{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./test/Wind-farm-energy-surplus-storage-solution-with-second-life-veh_2023_Energy-P.pdf\n",
      "Extraction and saving complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "# from openai import OpenAI\n",
    "\n",
    "# client = OpenAI(\n",
    "#   api_key=\"my-api-key\"\n",
    "# )\n",
    "# def summarize_text(content):\n",
    "#     response = client.chat.completions.create(\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     store=True,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": \"Summarize the following content:\\n\\n{content}\"}\n",
    "#     ]\n",
    "#     )\n",
    "#     return response.choices[0].text.strip()\n",
    "\n",
    "# Function to extract data from a single PDF as HTML\n",
    "def extract_pdf_as_html(pdf_path):\n",
    "    output_string = StringIO()\n",
    "    with open(pdf_path, 'rb') as fin:\n",
    "        extract_text_to_fp(\n",
    "            fin,\n",
    "            output_string,\n",
    "            laparams=LAParams(),\n",
    "            output_type='html',\n",
    "            codec=None,\n",
    "        )\n",
    "    return output_string.getvalue()\n",
    "\n",
    "# Function to parse HTML and extract structured data\n",
    "def parse_html_to_data(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    id_span = soup.find(\"span\", string=lambda text: text and \"EnergyPolicy\" in text)\n",
    "    id_value = id_span.text.strip() if id_span else \"ID not found\"\n",
    "\n",
    "    date_span = soup.find_all(string=lambda text: \"Availableonline\" in text)\n",
    "    raw_date = date_span[0].strip().replace(\"Availableonline\", \"\") if date_span else \"Date not found\"\n",
    "    try:\n",
    "        parsed_date = datetime.strptime(raw_date, \"%d%B%Y\")  # Parse \"20December2022\"\n",
    "        formatted_date = parsed_date.strftime(\"%d %B %Y\")    # Format to \"20 December 2022\"\n",
    "    except ValueError:\n",
    "        formatted_date = \"Invalid date format\"\n",
    "    # Extracting the title\n",
    "    title_span = soup.find('span', style=lambda s: s and \"font-family: CharisSIL\" in s and \"font-size:13px\" in s)\n",
    "    title = title_span.text.strip() if title_span else \"N/A\"\n",
    "\n",
    "    # Extracting authors\n",
    "    authors_spans = soup.find_all('span', style=lambda s: s and \"font-family: CharisSIL\" in s and \"font-size:10px\" in s)\n",
    "    authors = [span.text.strip() for span in authors_spans if span.text.strip()]\n",
    "    authors = \", \".join(authors) if authors else \"N/A\"\n",
    "\n",
    "    # Extracting content (from \"Introduction\" to \"References\")\n",
    "    intro_span = None\n",
    "    references_span = None\n",
    "    \n",
    "    # Find 'Introduction' span\n",
    "    for span in soup.find_all('span'):\n",
    "        if \"Introduction\" in span.text:\n",
    "            intro_span = span\n",
    "            break\n",
    "    \n",
    "    # Find 'References' span\n",
    "    for span in soup.find_all('span'):\n",
    "        if \"References\" in span.text:\n",
    "            references_span = span\n",
    "            break\n",
    "    content = \"\"\n",
    "    if intro_span and references_span:\n",
    "        # Get all elements after the intro_span\n",
    "        found_intro = False\n",
    "        content_sections = []\n",
    "        for element in soup.find_all():\n",
    "            # Start capturing content after the intro span\n",
    "            if element == intro_span:\n",
    "                found_intro = True\n",
    "                continue\n",
    "            # Stop capturing when references_span is reached\n",
    "            if element == references_span:\n",
    "                break\n",
    "            if found_intro and element.name == 'span':  # Ensure we only extract from span elements\n",
    "                content_sections.append(element.text.strip())\n",
    "        \n",
    "        content = \" \".join(content_sections).strip()\n",
    "    \n",
    "    # summary = \"\"\n",
    "    if not content:\n",
    "        content = \"Content not found\"\n",
    "    # else: \n",
    "        # summary = summarize_text(content)\n",
    "\n",
    "    # Final data structure\n",
    "    data = {\n",
    "        \"ID rada\": id_value.split(\")\")[1],\n",
    "        \"Datum\": formatted_date,\n",
    "        \"Naslov\": title,\n",
    "        \"Autori\": authors,\n",
    "        # \"Sažetak\": summary if content else \"No content or out of tokens\",\n",
    "        \"Sadržaj\": content,\n",
    "    }\n",
    "    return data\n",
    "\n",
    "\n",
    "# Main function to process all PDFs in a folder\n",
    "def process_pdfs_in_folder(folder_path):\n",
    "    all_data = []\n",
    "    for pdf_file in os.listdir(folder_path):\n",
    "        if pdf_file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder_path, pdf_file)\n",
    "            print(f\"Processing: {pdf_path}\")\n",
    "            \n",
    "            # Extract HTML content from the PDF\n",
    "            html_content = extract_pdf_as_html(pdf_path)\n",
    "            # Parse HTML content to structured data\n",
    "            parsed_data = parse_html_to_data(html_content)\n",
    "            parsed_data[\"File\"] = pdf_file  # Add filename for reference\n",
    "            all_data.append(parsed_data)\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "# Folder containing PDFs\n",
    "pdf_folder = \"./20\"  # Update this with the name of your folder\n",
    "data = process_pdfs_in_folder(pdf_folder)\n",
    "\n",
    "# Save extracted data to JSON and CSV\n",
    "output_folder = \"./output_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save as JSON\n",
    "json_path = os.path.join(output_folder, \"data.json\")\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Save as CSV\n",
    "df = pd.DataFrame(data)\n",
    "csv_path = os.path.join(output_folder, \"data.csv\")\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Extraction and saving complete!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
